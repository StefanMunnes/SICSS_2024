---
title: "Project Activism Framing GB"
date: July 15, 2024
format: 
  html:
    embed-resources: true
editor: visual
execute: 
  eval: false
  echo: true
  output: false
  warning: false
  error: false
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Data acquisition

## Packages

```{r}
library(tidyverse)
library(rvest)
library(stringi)
library(RSelenium)
```

## Scraping BBC

```{r}
scrape_article <- function(url) {
  article_page <- read_html(url)
  
  title <- article_page %>%
    #html_node("h1.headline") %>%
    html_node(".bWszMR") %>%
    html_text(trim = TRUE)
  
  author <- article_page %>%
    html_node(".hhBctz") %>%
    html_text(trim = TRUE)
  # Remove "By " and trailing comma from author name
  author <- gsub("By\\s+|,$", "", author)
  
  date_string <- article_page %>%
    html_node(".WPunI") %>%
    html_text(trim = TRUE)
  # Format date_str into appropriate form
  if (grepl("days ago", date_string) || grepl("day ago", date_string)) {
    # Handle relative date like "1 day ago", "2 days ago"...
    days_ago <- as.numeric(gsub("\\D", "", date_string))  
    date <- Sys.Date() - days_ago  # Calculate the date
  } else if (grepl("hrs ago", date_string) || grepl("hr ago", date_string)) {
    # Handle relative date like "1 hour ago", "2 hours ago"...
    hours_ago <- as.numeric(gsub("\\D", "", date_string))
    date <- Sys.time() - hours(1) * hours_ago 
  } else {
    # Handle date string like "25 June 2024"
    date <- dmy(date_string)  # Convert to Date object using lubridate
  }
  
  body <- article_page %>%
    html_nodes(".fYAfXe") %>%
    html_text(trim = TRUE) %>%
    paste(collapse = " ")
  
  tibble(
    title = title,
    author = author,
    date = date,
    body = body,
    url = url
  )
}

test_article <- scrape_article("https://www.bbc.com/news/articles/crge8vjg8y3o")

scrape_search_results <- function(search_url) {
  search_page <- read_html(search_url)
  
  article_urls <- search_page %>%
    html_nodes("a.sc-2e6baa30-0.gILusN") %>%
    html_attr("href") %>%
    unique() %>%
    .[grepl("/news/articles", .)] %>%
    paste0("https://www.bbc.com", .)
  
  return(article_urls)
}

# Test for first page
test_page <- scrape_search_results("https://www.bbc.com/search?q=%22just%20stop%20oil%22")
for (url in test_page){
  print(url)
}
```

## Scraping Just Stop Oil

```{r}
scrape_article_jso <- function(url) {
  article_page <- read_html(url)
  
  title <- article_page %>%
    html_node(".entry-title") %>%
    html_text(trim = TRUE)
  
  # No author in Just Stop Oil
    
  date_str <- article_page %>%
    html_node(".published") %>%
    #html_attr("datetime")
    html_text(trim = TRUE)
  # Convert string (e.g. July 10, 2024) to Datetime
  date <- mdy(date_str)
  
  body <- article_page %>%
    html_nodes("p") %>%
    html_text(trim = TRUE) %>%
    paste(collapse = " ")
  
  # Split the body text at "ENDS" and take the first part
  body <- strsplit(body, " ENDS", fixed = TRUE)[[1]][1]
  
  tibble(
    title = title,
    date = date,
    body = body,
    url = url
  )
}
# Testing JSO scraping
test_url <- "https://juststopoil.org/2024/07/11/5-just-stop-oil-supporters-found-guilty-as-un-slams-trial/"
output_test <- scrape_article_jso(test_url)
```

```{r}
# List to store all articles
#all_articles <- list()
#search_url <- "https://www.bbc.com/search?q=%22just%20stop%20oil%22"
#repeat {
  #print("Going to next page")
  
  # Get article URLs from the search results page
#  article_urls <- scrape_search_results(search_url)
  
  # Loop through each article URL and scrape the details
#  for (url in article_urls) {
#    print(paste("Scraping article:", url))
#    article_details <- scrape_article(url)
#    all_articles <- append(all_articles, list(article_details))
#    Sys.sleep(1)
#  }
#  Sys.sleep(1)
#  break
  # Check if max date is reached
  # Move to next page
#}

# Combine all articles into a single tibble
#combined_articles <- bind_rows(all_articles)
```

## Selenium BBC

```{r}
# Start RSelenium
rD <- rsDriver(browser = "chrome", port = 4444L, verbose = FALSE)
remDr <- rD$client

# Specify the search URL
search_url <- "https://www.bbc.com/search?q=%22just%20stop%20oil%22"

# Navigate to the search URL
remDr$navigate(search_url)

# Initialize list to store all articles
all_articles <- list()

# Specify the maximum date to stop scraping
max_date <- as.Date("2024-01-01") 

base_url <- "https://www.bbc.com"

# Loop to navigate through search result pages
while (TRUE) {
  # Get the page source and parse it
  page_source <- remDr$getPageSource()[[1]]
  search_page <- read_html(page_source)
  
  # Get article URLs from the search page
  article_urls <- search_page %>%
    html_nodes("a[href*='/2024']") %>%
    html_attr("href")

  # Generate full URLs
  full_urls <- paste0(base_url, article_urls)
  
  # Loop through each article URL and scrape the details
  for (url in full_urls) {
    print(paste("Scraping article:", url))
    article_details <- scrape_article(url)
    all_articles <- append(all_articles, list(article_details))
    Sys.sleep(1)
  }
  
  # Check the date of the last scraped article
  last_article_date <- min(as.Date(sapply(all_articles, function(x) x$date), na.rm = TRUE))
  
  # Check if last_article_date is NA or NULL
  if (is.na(last_article_date) || is.null(last_article_date)) {
    print("Error: Unable to determine last article date.")
    break
  }
  
  if (last_article_date < max_date) {
    print("Oldest date reached. Terminating scraping.")
    break
  }
  
  # Check if the "Next" button is available and click it
  next_button <- tryCatch({
    remDr$findElement(using = "css selector", "button[data-testid='pagination-next-button']")
  }, error = function(e) NULL)
  
  if (is.null(next_button)) {
    print("Could not find next button. Exiting loop.")
    break  # Exit loop if there is no "Next" button
  } else {
    print("Next page")
    next_button$clickElement()
    Sys.sleep(2)  # Wait for the next page to load
  }
}

# Stop RSelenium
remDr$close()
rD$server$stop()

# Combine all articles into a single tibble
combined_articles <- bind_rows(all_articles)

```

## Selenium JSO

```{r}
# Start RSelenium
rD2 <- rsDriver(browser = "chrome", port = 4448L, verbose = FALSE)
remDr2 <- rD2$client

# Specify the search URL
search_url_jso <- "https://juststopoil.org/news-press/"

# Navigate to the search URL
remDr2$navigate(search_url_jso)

# Initialize list to store all articles
all_articles_jso <- list()

# Specify the maximum date to stop scraping
max_date <- as.Date("2024-01-07") 
# Initialize button
load_more_button <- tryCatch({
  remDr2$findElement(using = "css selector", ".elementor-element-0af436e .elementor-button")
}, error = function(e) NULL)

# Loop to navigate through search result pages
while (TRUE) {
  # Get the page source and parse it
  page_source <- remDr2$getPageSource()[[1]]
  search_page <- read_html(page_source)
  
  # Check Date -> Click -> Check Date -> Stop -> Get URL
  # Just Stop Oil uses a button with class 'elementor-button' for "Load More"
  dates_str <- search_page %>%
    html_node(".elementor-post__meta-data span") %>%
    html_text(trim = TRUE)
    #html_node(".elementor-post__meta-data span") %>%
    
  dates <- mdy(dates_str)
  # Skip if the date text contains "No Comments"
  #if (grepl("No Comments", dates_str)) {
  #  return(NULL)  # Skip this article
  #}
  
  
  print("Dates")
  #print(dates_str)
  for (date in dates_str) {
    print(date)
  }
  print("Min Date so far")
  print(min(dates))
  
  if (min(dates) < max_date) {
    print("Final date reached!")
    break
  }
  
  if (is.null(load_more_button)) {
    print("Could not find Load More button. Exiting loop.")
    break  # Exit loop if there is no "Load More" button
  } else {
    print("Loading more articles")
    load_more_button$clickElement()
    Sys.sleep(2)  # Wait for the next page to load
  }
}

# Get article URLs from the search page
article_urls <- search_page %>%
  html_nodes("a[href*='/202']") %>%
  html_attr("href")

# Generate unique URLs
full_urls <- unique(article_urls)  # Since the URLs are already full on Just Stop Oil website

# Loop through each article URL and scrape the details
for (url in full_urls) {
  print(paste("Scraping article:", url))
  article_details <- scrape_article_jso(url)  # Adjusted function to scrape Just Stop Oil articles
  all_articles_jso <- append(all_articles_jso, list(article_details))
  Sys.sleep(1)
  }

# Stop RSelenium
remDr2$close()
rD2$server$stop()

# Combine all articles into a single tibble
combined_articles_jso <- bind_rows(all_articles_jso)
just_stop_oil_df <- combined_articles_jso %>%
  distinct() %>%
  filter(!is.na(title))
# Print combined articles
print(combined_articles_jso)

# Export the dataframe as a CSV file
write.csv(just_stop_oil_df, "dataframe_just_stop_oil.csv", row.names = FALSE)

```
