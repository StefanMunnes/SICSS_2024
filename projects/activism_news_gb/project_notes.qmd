---
title: "Project Activism Framing GB"
date: July 15, 2024
format: 
  html:
    embed-resources: true
editor: visual
execute: 
  eval: false
  echo: true
  output: false
  warning: false
  error: false
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Data acquisition

---
title: "Web Scraping Exercise"
date: July 9, 2024
format: 
  html:
    embed-resources: true
editor: visual
execute: 
  eval: false
  echo: true
  output: false
  warning: false
  error: false
---

## Packages

```{r}
library(tidyverse)
library(rvest)
library(stringi)
library(RSelenium)
#library(stringr)
```

## Scraping

```{r}
scrape_article <- function(url) {
  article_page <- read_html(url)
  
  title <- article_page %>%
    #html_node("h1.headline") %>%
    html_node(".bWszMR") %>%
    html_text(trim = TRUE)
  
  author <- article_page %>%
    html_node(".hhBctz") %>%
    html_text(trim = TRUE)
  # Remove "By " and trailing comma from author name
  author <- gsub("By\\s+|,$", "", author)
  
  date_string <- article_page %>%
    html_node(".WPunI") %>%
    html_text(trim = TRUE)
  # Format date_str into appropriate form
  if (grepl("days ago", date_string) || grepl("day ago", date_string)) {
    # Handle relative date like "5 days ago"
    days_ago <- as.numeric(gsub("\\D", "", date_string))  # Extract number of days
    date <- Sys.Date() - days_ago  # Calculate the date
  } else {
    # Handle date string like "25 June 2024"
    date <- dmy(date_string)  # Convert to Date object using lubridate
  }
  
  body <- article_page %>%
    html_nodes(".fYAfXe") %>%
    html_text(trim = TRUE) %>%
    paste(collapse = " ")
  
  tibble(
    title = title,
    author = author,
    date = date,
    body = body,
    url = url
  )
}

test_article <- scrape_article("https://www.bbc.com/news/articles/crge8vjg8y3o")

scrape_search_results <- function(search_url) {
  search_page <- read_html(search_url)
  
  article_urls <- search_page %>%
    html_nodes("a.sc-2e6baa30-0.gILusN") %>%
    html_attr("href") %>%
    unique() %>%
    .[grepl("/news/articles", .)] %>%
    paste0("https://www.bbc.com", .)
  
  return(article_urls)
}

test_page <- scrape_search_results("https://www.bbc.com/search?q=%22just%20stop%20oil%22")
for (url in test_page){
  print(url)
}
```

```{r}
# List to store all articles
all_articles <- list()
search_url <- "https://www.bbc.com/search?q=%22just%20stop%20oil%22"
repeat {
  #print("Going to next page")
  
  # Get article URLs from the search results page
  article_urls <- scrape_search_results(search_url)
  
  # Loop through each article URL and scrape the details
  for (url in article_urls) {
    # Skip subscriber articles
    #if (grepl("subscriber/article", url) || grepl("swampland.time", url)) {
    #  print(paste("Skipping article:", url))
    #  next
    #}
    print(paste("Scraping article:", url))
    article_details <- scrape_article(url)
    all_articles <- append(all_articles, list(article_details))
    Sys.sleep(1)
  }
  Sys.sleep(1)
  break
  # Check if max date is reached
  # Move to next page
}

# Combine all articles into a single tibble
combined_articles <- bind_rows(all_articles)
```

```{r}
# Start RSelenium
rD <- rsDriver(browser = "chrome", port = 4445L, verbose = FALSE)
remDr <- rD$client

# Specify the search URL
search_url <- "https://www.bbc.com/search?q=%22just%20stop%20oil%22"

# Navigate to the search URL
remDr$navigate(search_url)

# Initialize list to store all articles
all_articles <- list()
max_date <- as.Date("2023-01-01")  # Specify the maximum date to stop scraping

repeat {
  # Get the page source and parse it
  search_page <- remDr$getPageSource()[[1]] %>% read_html()
  
  # Get article URLs from the search results page
  article_urls <- scrape_search_results(search_page)
  
  # Loop through each article URL and scrape the details
  for (url in article_urls) {
    print(paste("Scraping article:", url))
    article_details <- scrape_article(url)
    all_articles <- append(all_articles, list(article_details))
    Sys.sleep(1)
  }
  
  # Check the date of the last scraped article
  last_article_date <- as.Date(max(sapply(all_articles, function(x) x$date), na.rm = TRUE))
  if (last_article_date < max_date) {
    break
  }
  
  # Check if the "Next" button is available and click it
  next_button <- tryCatch({
    remDr$findElement(using = "css selector", "button[data-testid='pagination-next-button']")
  }, error = function(e) NULL)
  
  if (is.null(next_button)) {
    break  # Exit loop if there is no "Next" button
  } else {
    next_button$clickElement()
    Sys.sleep(2)  # Wait for the next page to load
  }
}

# Stop RSelenium
remDr$close()
rD$server$stop()

# Combine all articles into a single tibble
combined_articles <- bind_rows(all_articles)
print(combined_articles)

```
