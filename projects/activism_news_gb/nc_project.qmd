---
title: "Project Activism Framing GB"
date: July 15, 2024
format: 
  html:
    embed-resources: true
editor: visual
execute: 
  eval: false
  echo: true
  output: false
  warning: false
  error: false
---

```{python}
import bbc
import datetime
#from .languages import Languages
from bbc import news
from bbc import exceptions

```

```{r}

file.edit("~/.Renviron")

Sys.getenv("newsapi_key")

```

```{r}
library(tidyverse)
library(httr)
library(jsonlite)
```

#Scraping NewsAPI

```{r}

httr_headlines <- GET("https://newsapi.org/v2/top-headlines/sources",
                      query = list(q= "just stop oil", country = "gb",
                                   apiKey = Sys.getenv("newsapi_key")))


# Extract the contents as JSON format/lists
#headlines <- content(httr_headlines, type='application/json', flatten = TRUE)

head3 <- fromJSON(content(httr_headlines, 
                             as = "text", 
                             encoding = "UTF-8"), 
                     flatten = TRUE)

article <- head3$articles

headlines1 <- content(httr_headlines, as="text")
fromJSON(headlines1) -> new

sources <- head(new$articles$source)

headlines <- content(httr_headlines, type='application/json', flatten = TRUE)$articles

headlines <- content(httr_headlines, type='application/json', flatten = TRUE)$sources

head2 <- content(httr_headlines, as = "parsed", type = "application/json")
#explicit convertion to data frame
dataFrame <- data.frame(head2)

```

```{r}

library(tidyverse)
library(rvest)
library(stringi)
library(RSelenium)

```

#Scraping the BBC

```{r}
scrape_article <- function(url) {
  article_page <- read_html(url)
  
  title <- article_page %>%
    #html_node("h1.headline") %>%
    html_node(".bWszMR") %>%
    html_text(trim = TRUE)
  
  author <- article_page %>%
    html_node(".hhBctz") %>%
    html_text(trim = TRUE)
  # Remove "By " and trailing comma from author name
  author <- gsub("By\\s+|,$", "", author)
  
  date_string <- article_page %>%
    html_node(".WPunI") %>%
    html_text(trim = TRUE)
  # Format date_str into appropriate form
  if (grepl("days ago", date_string) || grepl("day ago", date_string)) {
    # Handle relative date like "1 day ago", "2 days ago"...
    days_ago <- as.numeric(gsub("\\D", "", date_string))  
    date <- Sys.Date() - days_ago  # Calculate the date
  } else if (grepl("hrs ago", date_string) || grepl("hr ago", date_string)) {
    # Handle relative date like "1 hour ago", "2 hours ago"...
    hours_ago <- as.numeric(gsub("\\D", "", date_string))
    date <- Sys.time() - hours(1) * hours_ago 
  } else {
    # Handle date string like "25 June 2024"
    date <- dmy(date_string)  # Convert to Date object using lubridate
  }
  
  body <- article_page %>%
    html_nodes(".fYAfXe") %>%
    html_text(trim = TRUE) %>%
    paste(collapse = " ")
  
  tibble(
    title = title,
    author = author,
    date = date,
    body = body,
    url = url
  )
}

test_article <- scrape_article("https://www.bbc.com/news/articles/crge8vjg8y3o")
```

```{r}
#Used for testing 

scrape_search_results <- function(search_url) {
  search_page <- read_html(search_url)
  
  article_urls <- search_page %>%
    html_nodes("a.sc-2e6baa30-0.gILusN") %>%
    html_attr("href") %>%
    unique() %>%
    .[grepl("/news/articles", .)] %>%
    paste0("https://www.bbc.com", .)
  
  return(article_urls)
}

# Test for first page
test_page <- scrape_search_results("https://www.bbc.com/search?q=%22just%20stop%20oil%22")
for (url in test_page){
  print(url)
}

```

```{r}
# List to store all articles
all_articles <- list()
search_url <- "https://www.bbc.com/search?q=%22just%20stop%20oil%22"

# Get article URLs from the search results page
article_urls <- scrape_search_results(search_url)

article_urls <- unique(article_urls)
  
repeat {
  #print("Going to next page")
  
 
  
  # Loop through each article URL and scrape the details
  for (url in article_urls) {
    print(paste("Scraping article:", url))
    article_details <- scrape_article(url)
    all_articles <- append(all_articles, list(article_details))
    Sys.sleep(1)
  }
  Sys.sleep(1)
  break
  # Check if max date is reached
  # Move to next page
}

# Combine all articles into a single tibble
combined_articles <- bind_rows(all_articles)

{r}
# Start RSelenium
rD <- rsDriver(browser = "chrome", port = 4444L, verbose = FALSE)
remDr <- rD$client

# Specify the search URL
search_url <- "https://www.bbc.com/search?q=%22just%20stop%20oil%22"

# Navigate to the search URL
remDr$navigate(search_url)

# Initialize list to store all articles
all_articles <- list()

# Specify the maximum date to stop scraping
max_date <- as.Date("2024-01-01") 


##### Testing area #####
#ps_test <- remDr$getPageSource()[[1]]
#sp_test <- read_html(ps_test)

#article_urls <- sp_test %>%
#  html_nodes("a[href*='/news/articles/']") %>%
#  html_attr("href")

# Base URL
#base_url <- "https://www.bbc.com"

# Generate full URLs
#full_urls <- paste0(base_url, article_urls)

#for (url in full_urls) {
#  print(url)
#}

########################
base_url <- "https://www.bbc.com"

# Loop to navigate through search result pages
while (TRUE) {
  # Get the page source and parse it
  page_source <- remDr$getPageSource()[[1]]
  search_page <- read_html(page_source)
  
  # Get article URLs from the search page
  article_urls <- search_page %>%
    html_nodes("a[href*='/news/articles/']") %>%
    html_attr("href")

  # Generate full URLs
  full_urls <- paste0(base_url, article_urls)
  
  # Loop through each article URL and scrape the details
  for (url in full_urls) {
    print(paste("Scraping article:", url))
    article_details <- scrape_article(url)
    all_articles <- append(all_articles, list(article_details))
    Sys.sleep(1)
  }
  
  # Check the date of the last scraped article
  last_article_date <- min(as.Date(sapply(all_articles, function(x) x$date), na.rm = TRUE))
  
  # Check if last_article_date is NA or NULL
  if (is.na(last_article_date) || is.null(last_article_date)) {
    print("Error: Unable to determine last article date.")
    break
  }
  
  if (last_article_date < max_date) {
    print("Oldest date reached. Terminating scraping.")
    break
  }
  
  # Check if the "Next" button is available and click it
  next_button <- tryCatch({
    remDr$findElement(using = "css selector", "button[data-testid='pagination-next-button']")
  }, error = function(e) NULL)
  
  if (is.null(next_button)) {
    print("Could not find next button. Exiting loop.")
    break  # Exit loop if there is no "Next" button
  } else {
    print("Next page")
    next_button$clickElement()
    Sys.sleep(2)  # Wait for the next page to load
  }
}

# Stop RSelenium
remDr$close()
rD$server$stop()

# Combine all articles into a single tibble
combined_articles <- bind_rows(all_articles)
```

```{r}
#Scraping Just Stop Oil

scrape_article_jso <- function(url) {
  article_page <- read_html(url)
  
  title <- article_page %>%
    html_node(".entry-title") %>%
    html_text(trim = TRUE)
  
  # No author in Just Stop Oil
    
  date <- article_page %>%
    html_node("time") %>%
    html_attr("datetime")
  
  body <- article_page %>%
    html_nodes("p") %>%
    html_text(trim = TRUE) %>%
    paste(collapse = " ")
  
  tibble(
    title = title,
    date = date,
    body = body,
    url = url
  )
}

```

{r}

# Testing JSO scraping

```{r}
test_url <- "https://juststopoil.org/2024/07/11/5-just-stop-oil-supporters-found-guilty-as-un-slams-trial/"
output_test <- scrape_article_jso(test_url)
```

\`\`\`
