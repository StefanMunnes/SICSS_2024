# Now all we need is to 'build' the API call using httr
httr_bills <- GET(url = "https://api.congress.gov/v3/bill",
query = list(format = "json",
limit = 250,
offset = 0,
sort = "updateDate+desc",
api_key = Sys.getenv("congress_key")))
library(tidyverse)
library(httr)
# Now all we need is to 'build' the API call using httr
httr_bills <- GET(url = "https://api.congress.gov/v3/bill",
query = list(format = "json",
limit = 250,
offset = 0,
sort = "updateDate+desc",
api_key = Sys.getenv("congress_key")))
View(httr_bills)
View(httr_bills)
# Extract the contents as JSON format/lists
bills <- content(httr_bills, type='application/json')
View(bills)
httr_bills$content
View(bills)
# We extracted three elements, but we only need the bills list
bills <- content(httr_bills, type='application/json')$bills
View(bills)
# Now we can easily turn the list of lists into a DF, with the dplyr function
# bind_rows
df_bills <- bind_rows(bills)
View(df_bills)
# With purr package
library(purrr)
bills <- map(bills, flatten)
# Now we can easily turn the list of lists into a DF, with the dplyr function
# bind_rows
df_bills <- bind_rows(bills)
View(df_bills)
library(tidyverse)
library(httr)
file.edit("~/.Renviron")
# Add "congress_key = [your key]" and save
# Restart R, then you can access the key via
Sys.getenv("congress_key")
# Now all we need is to 'build' the API call using httr
httr_bills <- GET(url = "https://api.congress.gov/v3/bill",
query = list(format = "json",
limit = 250,
offset = 0,
sort = "updateDate+desc",
api_key = Sys.getenv("congress_key")))
httr_bills$content
# Extract the contents as JSON format/lists
bills <- content(httr_bills, type='application/json')
# We extracted three elements, but we only need the bills list
bills <- content(httr_bills, type='application/json')$bills
# Now we can easily turn the list of lists into a DF, with the dplyr function
# bind_rows
df_bills <- bind_rows(bills)
View(df_bills)
library(tidyverse)
library(rvest)
# that's it
title <- website %>%
html_node(".mw-page-title-main") %>%
html_text()
# First scrape the full page into R
website <- read_html("https://en.wikipedia.org/wiki/Computational_social_science")
# First scrape the full page into R
website <- read_html("https://en.wikipedia.org/wiki/Computational_social_science")
# that's it
title <- website %>%
html_node(".mw-page-title-main") %>%
html_text()
title
text <- website %>%
html_nodes("ul:nth-child(9) li , p") %>%
html_text() %>%
paste(collapse = "")
text
text <- website %>%
html_nodes("ul:nth-child(9) li , p") %>%
html_text()
text
text <- website %>%
html_nodes("ul:nth-child(9) li , p") %>%
html_text() %>%
paste(collapse = "")
url <- website %>%
html_node(".navigation-not-searchable .mw-redirect") %>%
html_attr("href")
url
# Lastly, it is very easy to combine this into a data frame right away, using tibble()
css_wiki <- tibble(
title = website %>%
html_node(".mw-page-title-main") %>%
html_text(),
text = website %>%
html_nodes("ul:nth-child(9) li , p") %>%
html_text() %>%
paste(collapse = ""),
url = website %>%
html_node(".navigation-not-searchable .mw-redirect") %>%
html_attr("href")
)
View(css_wiki)
```{r}
library(tidyverse)
library(rvest)
library(stringi)
website <- read_html("https://time.com/5759964/australian-bushfires-climate-change/")
title <- website %>%
html_node("#article-header .self-baseline") %>%
html_text()
byline <- website %>%
html_nodes("#article-body a.font-bold") %>%
html_text() %>%
paste(collapse = ";")
date <- website %>%
html_node("time") %>%
html_text()
body <- website %>%
html_nodes("#article-body p") %>%
html_text() %>%
paste(collapse = " ")
body
library(tidyverse)
library(tidyverse)
library(rvest)
library(stringi)
website <- read_html("https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702")
title <- website %>%
html_node(".ecl-page-header__title") %>%
html_text()
website <- read_html("https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702")
title <- website %>%
html_node(".ecl-page-header__title") %>%
html_text()
library(tidyverse)
library(rvest)
website <- read_xml("https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702")
install.packages(xml2)
install.packages("xml2")
install.packages("xml2")
library(xml2)
website <- read_xml("https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702")
website <- read_html("https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702")
website %>%
xml_structure()
test <- read_xml("https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702")
?read_xml
test <- read_xml(base_url = "https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702",
as_html = as_html)
website$node
website <- read_html("https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702")
title <- website %>%
xml_nodes(".ecl-page-header__title") %>%
xml_text()
title <- website %>%
html_elements(".ecl-page-header__title") %>%
xml_text()
?html_elements
website %>%
xml_structure()
website %>%
xml_ns_strip()
test <- website %>%
xml_ns_strip()
website %>%
html_text2()
website %>%
xml_contents()
website %>%
xml_contents()$body
website %>%
xml_contents()
test <- website %>%
xml_contents()
test[[2]]
test2 <- test[[2]]
View(test2)
test2[[1]]
test2[[2]]
test2[[3]]
test2$doc[[2]]
test2$doc
View(test2)
test2
test2 <- test[[2]] %>%
html_nodes(  )
test2 <- test[[2]] %>%
html_nodes()
test2 <- test[[2]] %>%
html_nodes(body)
test2 <- test[[2]] %>%
html_nodes("body")
test2 <- test[[2]]
xml_root(test2)
xml_child(test2)
test2 <- paste(test[[2]], collapse = TRUE)
paste(test2, collapse = TRUE)
paste(test2, collapse = "")
test2 <- paste(test[[2]], collapse = "")
test2
View(test)
test[[2]]$doc[[1]]
test[[2]]$doc
test[[2]]$node
