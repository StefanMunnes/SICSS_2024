---
title: "Text Analysis I - Exercise I"
date: July 11, 2024
format: 
  html:
    embed-resources: true
    number-sections: true
editor: visual
execute: 
  eval: false
  echo: true
  output: false
  warning: false
  error: false
---


# Preparation

Packages you will (probably) need:
```{r}
library(dplyr)
library(quanteda)
library(quanteda.sentiment)
library(seededlda)
library(quanteda.textmodels)
```

Use the script from exercise 1 to get all the necessary objects for the analysis


# Perform a sentiment analysis

Get a dictionary from the quanteda.sentiment package and calculate the polarity (or valence) sentiment for each document:
```{r}
dict_pol <- data_dictionary_HuLiu

dfm_lookup(dfm_pp, dict_pol)

polarity <- textstat_polarity(dfm_pp, dict_pol)

valence <- textstat_valence(dfm_pp, data_dictionary_AFINN)
```


# Perform a topic models analysis

Use the quantda.texmodel function for a explorativ approach or define some seeds if you expect some topics with seeded lda:
```{r}
tmod_lda <- textmodel_lda(dfm_pp[1:100, ], k = 5)

terms(tmod_lda, 10)

topics(tmod_lda)
```


# **Bonus**: Train your own naive bayes classifier for sentiment 

Get a training set from the labeled data and define classes:
```{r}
dfm_pp_train <- dfm_pp[1:200, ]

sentiment_class <- cut(
  polarity$sentiment,
  breaks = c(-1, -0.2, 0.2, 1),
  labels = c("negative", "neutral", "positive"))

sentiment_class_train <- sentiment_class[1:200]
```

Train the model with the training data and classes:
```{r}
model_nb <- textmodel_nb(dfm_pp_train, sentiment_class)
```

Get (unlabelled) test data:
```{r}
dfm_pp_test <- dfm_pp[201:300, ]
```

Predict class for test data with trained model:
```{r}
predictions_sent <- predict(model_nb, dfm_pp_test)
```

Create a simple confusion matrix to check prediction accuracy:
```{r}
table(Predicted = predictions_sent, Actual = sentiment_class[201:300])
```