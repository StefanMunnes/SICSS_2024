keyness_result <- textstat_keyness(dfm, target = "fox.1")
textplot_keyness(keyness_result)
tokens <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en"), padding = TRUE)
dfm <- dfm(tokens)
docnames(dfm) <- data_news$source
keyness_result <- textstat_keyness(dfm, target = "fox.1")
textplot_keyness(keyness_result)
keyness_result <- textstat_keyness(dfm, target = "fox.2")
textplot_keyness(keyness_result)
corpus <- corpus(data_news, text_field = "body")
tokens <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en"), padding = TRUE)
dfm <- dfm(tokens, groups = data_news$source)
keyness_result <- textstat_keyness(dfm, target = "fox")
textplot_keyness(keyness_result)
stopwords("en")
?paste0
stopwords("en") <- paste0(stopwords("en"), "fox", "news", "getti", "image")
chr(stopwords("en"))
?c
stopwords("en") <- c(stopwords("en"), "fox", "news", "getti", "image")
c("la", "le")
c(stopwords("en"))
c(stopwords("en"), "test")
new_list <- c(stopwords("en"), "fox", "news", "getti", "image")
tokens <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_wordstem(language = "en") |>
tokens_remove(new_list, padding = TRUE)
dfm <- dfm(tokens, groups = data_news$source)
keyness_result <- textstat_keyness(dfm, target = "fox")
textplot_keyness(keyness_result)
tokens <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm <- dfm(tokens, groups = data_news$source)
keyness_result <- textstat_keyness(dfm, target = "fox")
textplot_keyness(keyness_result)
tokens <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
ALSE)
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click")
tokens <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm <- dfm(tokens, groups = data_news$source)
keyness_result <- textstat_keyness(dfm, target = "fox")
textplot_keyness(keyness_result)
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click", "via")
tokens <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm <- dfm(tokens, groups = data_news$source)
keyness_result <- textstat_keyness(dfm, target = "fox")
textplot_keyness(keyness_result)
corpus <- corpus(data_news, text_field = "body")
```{r}
tokens <- tokens(corpus, what = "word")
m
m
dfm <- dfm(tokens)
textstat_keyness(dfm, target = docvars(corpus, "source") == "cnn") |>
textplot_keyness()
textstat_keyness(dfm_pp, target = docvars(corpus, "source") == "cnn") |>
textplot_keyness()
textstat_keyness(dfm_pp, target = docvars(corpus, "source") == "fox") |>
textplot_keyness()
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click", "via")
tokens <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm <- dfm(tokens, groups = data_news$source)
keyness_result <- textstat_keyness(dfm, target = "fox")
textplot_keyness(keyness_result)
tmod_lda <- textmodel_lda(dfm_pp, k = 3)
library(seededlda)
tmod_lda <- textmodel_lda(dfm_pp, k = 3)
terms(n=10)
terms(tmod_lda)
tmod_lda <- textmodel_lda(dfm_pp, k = 5)
terms(tmod_lda)
topics(tmod_lda)
?slice_sample
training <- slice_sample(n = 250, by = "source")
training <- slice_sample(data_news, n = 250, by = "source")
library(quanteda.textmodels)
training <- slice_sample(data_news, n = 250, by = "source")
training <- corpus(slice_sample(data_news, n = 250, by = "source"))
training <- slice_sample(data_news, n = 250, by = "source")
corpus <- corpus(training)
corpus <- corpus(training, text_field = "body")
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click", "via")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm_pp <- dfm(tok_pp)
tmod_nb <- textmodel_nb(dfm_pp, source)
?textmodel_nb
class <- training$source
tmod_nb <- textmodel_nb(dfm_pp, class)
summary(tmod_nb)
library(quanteda.sentiment)
summary(tmod_nb)
test <- data_news %>%
filter(!body %in% training)
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
getwd()
setwd("D:/SICSS_2024/exercises/Armin/day4_text_analysis_1")
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
getwd()
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
training <- slice_sample(data_news, n = 250, by = "source")
corpus <- corpus(training, text_field = "body")
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click", "via")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm_pp <- dfm(tok_pp)
class <- training$source
tmod_nb <- textmodel_nb(dfm_pp, class)
summary(tmod_nb)
test <- data_news %>%
filter(!body %in% training$body)
test <- data_news %>%
filter(!body %in% training$body) %>%
slice_sample(n=400)
tok_test <- tokens(
test,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
corp_test <- corpus(test, text_field = "body")
tok_test <- tokens(
test,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
tok_test <- tokens(
corp_test,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm_test <- dfm(tok_test)
predicted_class <- predict(tmod_nb, dfm_test)
predicted_class <- predict(tmod_nb, dfm_test$body)
tab_class <- table(class, predicted_class)
caret::confusionMatrix(tab_class, mode = "everything")
library(caret)
install.packages("caret")
library(caret)
caret::confusionMatrix(tab_class, mode = "everything")
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click", "via")
get_dfm <- function(x){
corpus <- corpus(x, text_field = "body")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm_pp <- dfm(tok_pp)
}
get_dfm <- function(x){
corpus <- corpus(x, text_field = "body")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm_pp <- dfm(tok_pp)
return(dfm_pp)
}
train_dfm <- slice_sample(data_news, n = 250, by = "source") %>%
function(x)
train_dfm <- slice_sample(data_news, n = 250, by = "source") %>%
get_dfm()
class <- training$source
tmod_nb <- textmodel_nb(dfm_pp, class)
test_dfm <- data_news %>%
filter(!body %in% train_dfm)
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click", "via")
corpus <- corpus(data_news, text_field = "body")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm_pp <- dfm(tok_pp)
train_dfm <- slice_sample(dfm_pp)
d
train_dfm <- dfm_sample(dfm_pp, size = 250, by = dfm_pp@docvars$source)
test_dfm <- dfm_pp %in% train_dfm
test_dfm <- train_dfm(dfm_pp, docnames(dfm_pp) %in% test_dfm)
test_dfm <- dfm_subset(dfm_pp, docnames(dfm_pp) %in% test_dfm)
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% train_docs)
train_docs <- docnames(train_dfm)
# Create a new DFM that excludes the documents in the sample
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% train_docs)
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% docnames(train_dfm))
tmod_nb <- textmodel_nb(train_dfm, dfm_pp@docvars$source)
tmod_nb <- textmodel_nb(train_dfm, train_dfm@docvars$source)
predicted_class <- predict(tmod_nb, test_dfm)
tab_class <- table(class, predicted_class)
tab_class <- table(test_dfm@docvars$source, predicted_class)
caret::confusionMatrix(tab_class, mode = "everything")
View(dfm_pp)
textstat_frequency(dfm_pp, n=20)
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click", "via", "")
corpus <- corpus(data_news, text_field = "body")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE) |>
tokens_wordstem(language = "en")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE) |>
tokens_wordstem(language = "en")
dfm_pp <- dfm(tok_pp)
train_dfm <- dfm_sample(dfm_pp, size = 250, by = dfm_pp@docvars$source)
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% docnames(train_dfm))
tmod_nb <- textmodel_nb(train_dfm, train_dfm@docvars$source)
predicted_class <- predict(tmod_nb, test_dfm)
tab_class <- table(test_dfm@docvars$source, predicted_class)
caret::confusionMatrix(tab_class, mode = "everything")
matr_stem <- caret::confusionMatrix(tab_class, mode = "everything")
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click", "via", "")
corpus <- corpus(data_news, text_field = "body")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm_pp <- dfm(tok_pp)
train_dfm <- dfm_sample(dfm_pp, size = 250, by = dfm_pp@docvars$source)
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% docnames(train_dfm))
tmod_nb <- textmodel_nb(train_dfm, train_dfm@docvars$source)
predicted_class <- predict(tmod_nb, test_dfm)
tab_class <- table(test_dfm@docvars$source, predicted_class)
tab_class <- table(test_dfm@docvars$source, predicted_class)
matr_stem <- caret::confusionMatrix(tab_class, mode = "everything")
`
corpus <- corpus(data_news, text_field = "body")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(stopwords("en"), padding = FALSE)
dfm_pp <- dfm(tok_pp)
train_dfm <- dfm_sample(dfm_pp, size = 250, by = dfm_pp@docvars$source)
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% docnames(train_dfm))
tmod_nb <- textmodel_nb(train_dfm, train_dfm@docvars$source)
predicted_class <- predict(tmod_nb, test_dfm)
tab_class <- table(test_dfm@docvars$source, predicted_class)
corpus <- corpus(data_news, text_field = "body")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(stopwords("en"), padding = FALSE)
dfm_pp <- dfm(tok_pp)
train_dfm <- dfm_sample(dfm_pp, size = 250, by = dfm_pp@docvars$source)
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% docnames(train_dfm))
tmod_nb <- textmodel_nb(train_dfm, train_dfm@docvars$source)
predicted_class <- predict(tmod_nb, test_dfm)
tab_class <- table(test_dfm@docvars$source, predicted_class)
matr_normstopwords <- caret::confusionMatrix(tab_class, mode = "everything")
View(dfm_test)
View(matr_normstopwords)
View(matr_stem)
matr_normstopwords
matr_stem
set.seed(161)
new_list <- c(stopwords("en"), "fox", "news", "getty", "images", "cnn", "digital", "photo", "click", "via")
corpus <- corpus(data_news, text_field = "body")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(stopwords("en"), padding = FALSE)
dfm_pp <- dfm(tok_pp)
dfm_pp <- dfm(tok_pp)
train_dfm <- dfm_sample(dfm_pp, size = 250, by = dfm_pp@docvars$source)
train_dfm <- dfm_sample(dfm_pp, size = 250, by = dfm_pp@docvars$source)
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% docnames(train_dfm))
tmod_nb <- textmodel_nb(train_dfm, train_dfm@docvars$source)
predicted_class <- predict(tmod_nb, test_dfm)
tab_class <- table(test_dfm@docvars$source, predicted_class)
matr_normstopwords <- caret::confusionMatrix(tab_class, mode = "everything")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE)
dfm_pp <- dfm(tok_pp)
train_dfm <- dfm_sample(dfm_pp, size = 250, by = dfm_pp@docvars$source)
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% docnames(train_dfm))
tmod_nb <- textmodel_nb(train_dfm, train_dfm@docvars$source)
predicted_class <- predict(tmod_nb, test_dfm)
tab_class <- table(test_dfm@docvars$source, predicted_class)
tab_class <- table(test_dfm@docvars$source, predicted_class)
matr_nostem <- caret::confusionMatrix(tab_class, mode = "everything")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE) |>
tokens_wordstem(language = "en")
tok_pp <- tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(new_list, padding = FALSE) |>
tokens_wordstem(language = "en")
dfm_pp <- dfm(tok_pp)
train_dfm <- dfm_sample(dfm_pp, size = 250, by = dfm_pp@docvars$source)
test_dfm <- dfm_subset(dfm_pp, !docnames(dfm_pp) %in% docnames(train_dfm))
tmod_nb <- textmodel_nb(train_dfm, train_dfm@docvars$source)
predicted_class <- predict(tmod_nb, test_dfm)
tab_class <- table(test_dfm@docvars$source, predicted_class)
matr_stem <- caret::confusionMatrix(tab_class, mode = "everything")
matr_normstopwords
matr_nostem
matr_stem
