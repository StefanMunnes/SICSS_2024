library(dplyr)
library(reticulate)
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
set.seed(161)
data_news <- slice_sample(data_news, n = 20)
Sys.getenv("API_key")
?tilde
file.edit(~"/.Renviron")
file.edit(~"/.Renvion")
file.edit("~/.Renviron")
Sys.getenv("API_key")
Sys.getenv("API_key")
Sys.getenv("congress_key")
Sys.getenv("API_key")
Sys.getenv("API_key")
Sys.getenv("api_key")
library(dplyr)
library(reticulate)
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
set.seed(161)
data_news <- slice_sample(data_news, n = 20)
file.edit("~/.Renviron")
Sys.getenv("api_key")
library(dplyr)
library(reticulate)
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
set.seed(161)
data_news <- slice_sample(data_news, n = 20)
file.edit("~/.Renviron")
Sys.getenv("api_key")
reticulate::repl_python()
library(dplyr)
library(reticulate)
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
set.seed(161)
data_news <- slice_sample(data_news, n = 20)
file.edit("~/.Renviron")
Sys.getenv("api_key")
reticulate::repl_python()
library(reticulate)
use_python("/Users/ilaria.vitulano/miniconda3/python.exe")
use_python("/Users/ilaria.vitulano/miniconda3")
where python
View(data_cnn)
# save datanews into a csv
"/Users/ilaria.vitulano/Documents/Weizenbaum/Learning/SICSS/SICSS_2024/exercises/Ilaria_new/day5/exercises/data/CNNsample.csv", row.names = FALSE)
# save datanews into a csv
write.csv(data_news, file ="/Users/ilaria.vitulano/Documents/Weizenbaum/Learning/SICSS/SICSS_2024/exercises/Ilaria_new/day5/exercises/data/CNNsample.csv", row.names = FALSE)
Store the full key in your local environment and load with `{r}Sys.getenv("openai")`
library(readxl)
library(tidyverse)
library(vcd)
install.packages("grid")
install.packages("grid")
library(vcd)
library(irrCAC)
library(knitr)
Ilaria <- read_excel("/Users/ilaria.vitulano/Documents/Weizenbaum/Work/ChatGPT_2 Project/Intercoder reliability check/agreement_check_IlariaMykola_12Jul24.xlsx",
sheet = "Coder_Ilaria")
Mykola <- read_excel("/Users/ilaria.vitulano/Documents/Weizenbaum/Work/ChatGPT_2 Project/Intercoder reliability check/agreement_check_IlariaMykola_12Jul24.xlsx",
sheet = "Mykola")
Ilaria2 <- Ilaria[, c("id",  "text", "assessment")]
View(Ilaria2)
Mykola2 <- Mykola[, c("id",  "text", "assessment")]
dat_merged <- left_join(Ilaria, Mykola, by = "id")
View(dat_merged)
dat_merged <- left_join(Ilaria2, Mykola2, by = "id")
View(dat_merged)
ct_assessment <- table(dat_merged$assessment.x, dat_merged$assessment.y)
ct_assessment
K_assessment <- Kappa(ct_assessment)
K_assessment
krippen2.table(ct_assessment)
bp2.table(ct_assessment)
Ilaria <- read_excel("/Users/ilaria.vitulano/Documents/Weizenbaum/Work/ChatGPT_2 Project/Intercoder reliability check/agreement_check_IlariaMykola_12Jul24.xlsx",
sheet = "Coder_Ilaria")
Mykola <- read_excel("/Users/ilaria.vitulano/Documents/Weizenbaum/Work/ChatGPT_2 Project/Intercoder reliability check/agreement_check_IlariaMykola_12Jul24.xlsx",
sheet = "Mykola")
View(Ilaria)
View(Mykola)
View(Ilaria)
Ilaria2 <- Ilaria[, c("id",  "text", "assessment")]
Mykola2 <- Mykola[, c("id",  "text", "assessment")]
View(Ilaria)
Ilaria2 <- Ilaria[, c("n_claim",  "text", "assessment")]
Mykola2 <- Mykola[, c("n_claim",  "text", "assessment")]
dat_merged <- left_join(Ilaria2, Mykola2, by = "id")
dat_merged <- left_join(Ilaria2, Mykola2, by = "n_claim")
View(dat_merged)
View(Ilaria2)
View(Ilaria)
View(Mykola)
View(dat_merged)
ct_assessment <- table(dat_merged$assessment.x, dat_merged$assessment.y)
ct_assessment
table(dat_merged$assessment.x, dat_merged$assessment.y, exclude = NULL)
K_assessment <- Kappa(ct_assessment)
K_assessment
krippen2.table(ct_assessment)
bp2.table(ct_assessment)
load("/Users/ilaria.vitulano/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total.RData")
View(GESIS_filtered)
library(tydiverse)
# loading dataset with chatgpt ratings but with no topics
load("/Users/ilaria.vitulano/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total.RData")
# loading dataset (by tomas) with topics but no chatgpt ratings
load("/Users/ilaria.vitulano/Documents/Weizenbaum/Work/ChatGPT_2 Project/Analysis/Analysis_12Jul24/merged_GESIS_filtered_with_topic_labels.csv")
library(readr)
# loading dataset (by tomas) with topics but no chatgpt ratings
tomas_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Analysis/Analysis_12Jul24/merged_GESIS_filtered_with_topic_labels.csv")
# loading dataset with chatgpt ratings but with no topics
ilaria_df <- load("/Users/ilaria.vitulano/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total.RData")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
# loading dataset with chatgpt ratings but with no topics
ilaria_df <- load("/Users/ilaria.vitulano/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total.RData")
# loading dataset (by tomas) with topics but no chatgpt ratings
tomas_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Analysis/Analysis_12Jul24/merged_GESIS_filtered_with_topic_labels.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
# loading dataset with chatgpt ratings but with no topics
ilaria_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total_v2.csv")
# loading dataset (by tomas) with topics but no chatgpt ratings
tomas_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Analysis/Analysis_12Jul24/merged_GESIS_filtered_with_topic_labels.csv")
names(tomas_df)
# overwriting Tomas dataset, cutting all the columns, leaving only the ones relative to topic modeling (to avoid double columns in the merged dataset)
tomas_df <- tomas_df[, c("n_claim", "language_detected_cld2", "topic", "probability", "Topic", "Topic Name", "Topic Representation", "Human_Readable_Topic")]
View(ilaria_df)
# loading dataset with chatgpt ratings but with no topics
ilaria_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total_v2.csv")
# loading dataset with chatgpt ratings but with no topics
ilaria_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total_v2.csv")
# loading dataset with chatgpt ratings but with no topics
ilaria_df <- read_csv("/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total.csv")
# loading dataset with chatgpt ratings but with no topics
ilaria_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total.csv")
# loading dataset (by tomas) with topics but no chatgpt ratings
tomas_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Analysis/Analysis_12Jul24/merged_GESIS_filtered_with_topic_labels.csv")
names(tomas_df)
# overwriting Tomas dataset, cutting all the columns, leaving only the ones relative to topic modeling (to avoid double columns in the merged dataset)
tomas_df <- tomas_df[, c("n_claim", "language_detected_cld2", "topic", "probability", "Topic", "Topic Name", "Topic Representation", "Human_Readable_Topic")]
View(tomas_df)
# merging the two datasets
dat_total <- left_join(ilaria_df, tomas_df, by = "n_claim")
View(dat_total)
View(dat_total)
View(dat_total)
dat_total$Human_Readable_Topic <- paste0(dat_total$Topic, " ", dat_total$Human_Readable_Topic)
View(dat_total)
dat_total$Human_Readable_Topic <- paste0(dat_total$Topic, " .", dat_total$Human_Readable_Topic)
dat_total$Human_Readable_Topic <- paste0(dat_total$Topic, ". ", dat_total$Human_Readable_Topic)
View(dat_total)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
# loading dataset with chatgpt ratings but with no topics
ilaria_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total.csv")
# loading dataset (by tomas) with topics but no chatgpt ratings
tomas_df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Analysis/Analysis_12Jul24/merged_GESIS_filtered_with_topic_labels.csv")
# overwriting Tomas dataset, cutting all the columns, leaving only the ones relative to topic modeling (to avoid double columns in the merged dataset)
tomas_df <- tomas_df[, c("n_claim", "language_detected_cld2", "topic", "probability", "Topic", "Topic Name", "Topic Representation", "Human_Readable_Topic")]
# merging the two datasets
dat_total <- left_join(ilaria_df, tomas_df, by = "n_claim")
dat_total$Human_Readable_Topic <- paste0(dat_total$Topic, ". ", dat_total$Human_Readable_Topic)
View(dat_total)
unique(dat_total$Human_Readable_Topic)
table(dat_total$Human_Readable_Topic)
table <- table(dat_total$Human_Readable_Topic)
# loading dataset with chatgpt ratings but with no topics
df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
%>%
names(df)
table(df$GPT_rating)
df <- df %>% filter(GPT_rating != "N/A")
View(df)
table(df$GPT_rating)
ct <- table(df$ratingName, df$GPT_rating)
ct
?table
ct <- margin.table(df$ratingName, df$GPT_rating)
ct <- table(df$ratingName, df$GPT_rating, dnn = c("gesis", "gpt"))
ct
prop.table(ct, 1)*100
round(prop.table(ct, 1)*100, 2)
df %>% group_by(ratingName) %>% count(GPT_rating)
df %>% group_by(ratingName) %>% count(GPT_rating) %>% mutate(percentage=n/sum(n)*100,
perc_fig1 <- df %>% group_by(ratingName) %>% count(GPT_rating) %>% mutate(percentage=n/sum(n)*100, percentage_round=round(percentage,0))
View(perc_fig1)
perc_fig1
round(prop.table(ct, 1)*100, 2)
View(perc_fig1)
ggplot(perc_fig1, aes(x=percentage,y=ratingName,fill=GPT_rating)) +
geom_bar(stat="identity")
ggplot(perc_fig1, aes(x=percentage,y=ratingName,fill=GPT_rating)) +
geom_bar(stat="identity") + scale_fill_manual("GPT4 rating",values=c("mediumblue","cadetblue1", "magenta3")) +
scale_x_continuous(expand=c(0,0)) + ylab("GESIS ratings") + xlab("Percentage (%)") + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank())
ggplot(perc_fig1, aes(x=percentage,y=ratingName,fill=GPT_rating)) +
geom_bar(stat="identity") + scale_fill_manual("GPT4 rating",values=c("#8491B4B2","#F39B7FB2", "#DC0000B2")) +
scale_x_continuous(expand=c(0,0)) + ylab("GESIS ratings") + xlab("Percentage (%)") + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank())
ggplot(perc_fig1, aes(x=percentage,y=ratingName,fill=GPT_rating)) +
geom_bar(stat="identity", position = position_dodge()) + scale_fill_manual("GPT4 rating",values=c("#8491B4B2","#F39B7FB2", "#DC0000B2")) +
scale_x_continuous(expand=c(0,0)) + ylab("GESIS ratings") + xlab("Percentage (%)") + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank())
ggplot(perc_fig1, aes(x=percentage,y=ratingName,fill=GPT_rating)) +
geom_bar(stat="identity") + scale_fill_manual("GPT4 rating",values=c("#8491B4B2","#F39B7FB2", "#DC0000B2")) +
scale_x_continuous(expand=c(0,0)) + ylab("GESIS ratings") + xlab("Percentage (%)") + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank())
ggplot(perc_fig1, aes(x=percentage,y=ratingName,fill=GPT_rating)) +
geom_bar(stat="identity") + scale_fill_manual("GPT4 rating",values=c("#8491B4B2","#F39B7FB2", "#DC0000B2")) +
scale_x_continuous(expand=c(0,0)) + ylab("GESIS ratings") + xlab("Percentage (%)") + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.text.y = element_text(angle = 90))
View(perc_fig1)
View(df)
View(perc_fig1)
ct <- table(df$ratingName, df$GPT_rating, dnn = c("gesis", "gpt"))
ct
diag(ct)
sum(diag(ct))
sum(diag(ct))/sum(ct)
(sum(diag(ct))/sum(ct))*100
round((sum(diag(ct))/sum(ct))*100)
round((sum(diag(ct))/sum(ct))*100, 2)
?Kappa
Kappa(ct)
# Chunk 1: setup
knitr::opts_chunk$set(echo = FALSE, comment = NA)
library(tidyverse)
library(readr)
library(vcd)
# loading dataset with chatgpt ratings but with no topics
df <- read_csv("~/Documents/Weizenbaum/Work/ChatGPT_2 Project/Auditing_jul03/Output batches jul03/GESIS_GPToutput_total.csv")
# excluding the NA from the dataset (2194 NAs)
df <- df %>% filter(GPT_rating != "N/A")
# Chunk 2
ct <- table(df$ratingName, df$GPT_rating, dnn = c("gesis", "gpt"))
ct
# Chunk 3
round((sum(diag(ct))/sum(ct))*100, 2)
# Chunk 4
Kappa(ct)
# Chunk 5
round(prop.table(ct, 1)*100, 2)
# create table for the percentages (in a format that is compatible with ggplot)
perc_fig1 <- df %>% group_by(ratingName) %>% count(GPT_rating) %>% mutate(percentage=n/sum(n)*100, percentage_round=round(percentage,0))
ggplot(perc_fig1, aes(x=percentage,y=ratingName,fill=GPT_rating)) +
geom_bar(stat="identity") + scale_fill_manual("GPT4 rating",values=c("#8491B4B2","#F39B7FB2", "#DC0000B2")) +
scale_x_continuous(expand=c(0,0)) + ylab("GESIS ratings") + xlab("Percentage (%)") + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.text.y = element_text(angle = 90))
ct <- table(df$ratingName, df$GPT_rating, dnn = c("gesis", "gpt"))
ct
160+749+533
5542+1083+343
