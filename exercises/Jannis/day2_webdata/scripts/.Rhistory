# Now all we need is to 'build' the API call using httr
httr_bills <- GET(url = "https://api.congress.gov/v3/bill",
query = list(format = "json",
limit = 250,
offset = 0,
sort = "updateDate+desc",
api_key = Sys.getenv("congress_key")))
library(tidyverse)
library(httr)
# Now all we need is to 'build' the API call using httr
httr_bills <- GET(url = "https://api.congress.gov/v3/bill",
query = list(format = "json",
limit = 250,
offset = 0,
sort = "updateDate+desc",
api_key = Sys.getenv("congress_key")))
View(httr_bills)
View(httr_bills)
# Extract the contents as JSON format/lists
bills <- content(httr_bills, type='application/json')
View(bills)
httr_bills$content
View(bills)
# We extracted three elements, but we only need the bills list
bills <- content(httr_bills, type='application/json')$bills
View(bills)
# Now we can easily turn the list of lists into a DF, with the dplyr function
# bind_rows
df_bills <- bind_rows(bills)
View(df_bills)
# With purr package
library(purrr)
bills <- map(bills, flatten)
# Now we can easily turn the list of lists into a DF, with the dplyr function
# bind_rows
df_bills <- bind_rows(bills)
View(df_bills)
library(tidyverse)
library(httr)
file.edit("~/.Renviron")
# Add "congress_key = [your key]" and save
# Restart R, then you can access the key via
Sys.getenv("congress_key")
# Now all we need is to 'build' the API call using httr
httr_bills <- GET(url = "https://api.congress.gov/v3/bill",
query = list(format = "json",
limit = 250,
offset = 0,
sort = "updateDate+desc",
api_key = Sys.getenv("congress_key")))
httr_bills$content
# Extract the contents as JSON format/lists
bills <- content(httr_bills, type='application/json')
# We extracted three elements, but we only need the bills list
bills <- content(httr_bills, type='application/json')$bills
# Now we can easily turn the list of lists into a DF, with the dplyr function
# bind_rows
df_bills <- bind_rows(bills)
View(df_bills)
library(tidyverse)
library(rvest)
# that's it
title <- website %>%
html_node(".mw-page-title-main") %>%
html_text()
# First scrape the full page into R
website <- read_html("https://en.wikipedia.org/wiki/Computational_social_science")
# First scrape the full page into R
website <- read_html("https://en.wikipedia.org/wiki/Computational_social_science")
# that's it
title <- website %>%
html_node(".mw-page-title-main") %>%
html_text()
title
text <- website %>%
html_nodes("ul:nth-child(9) li , p") %>%
html_text() %>%
paste(collapse = "")
text
text <- website %>%
html_nodes("ul:nth-child(9) li , p") %>%
html_text()
text
text <- website %>%
html_nodes("ul:nth-child(9) li , p") %>%
html_text() %>%
paste(collapse = "")
url <- website %>%
html_node(".navigation-not-searchable .mw-redirect") %>%
html_attr("href")
url
# Lastly, it is very easy to combine this into a data frame right away, using tibble()
css_wiki <- tibble(
title = website %>%
html_node(".mw-page-title-main") %>%
html_text(),
text = website %>%
html_nodes("ul:nth-child(9) li , p") %>%
html_text() %>%
paste(collapse = ""),
url = website %>%
html_node(".navigation-not-searchable .mw-redirect") %>%
html_attr("href")
)
View(css_wiki)
```{r}
library(tidyverse)
library(rvest)
library(stringi)
website <- read_html("https://time.com/5759964/australian-bushfires-climate-change/")
title <- website %>%
html_node("#article-header .self-baseline") %>%
html_text()
byline <- website %>%
html_nodes("#article-body a.font-bold") %>%
html_text() %>%
paste(collapse = ";")
date <- website %>%
html_node("time") %>%
html_text()
body <- website %>%
html_nodes("#article-body p") %>%
html_text() %>%
paste(collapse = " ")
body
library(tidyverse)
library(tidyverse)
library(rvest)
library(stringi)
website <- read_html("https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702")
title <- website %>%
html_node(".ecl-page-header__title") %>%
html_text()
website <- read_html("https://ec.europa.eu/commission/presscorner/detail/en/ip_24_3702")
title <- website %>%
html_node(".ecl-page-header__title") %>%
html_text()
