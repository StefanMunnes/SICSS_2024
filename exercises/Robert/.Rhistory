str_extract(test_string, "(\d+)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(\\d+)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(\\.+)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(.+)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "^.+")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(^.+)\;$")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(^.+)\\;$")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "^(.+)\\;$")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "^(.+);$")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(^.+)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(.+)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(\\w\\b$)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(\\w+\\b$)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(\\W+\\b$)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "\\W+\\b$")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "\\.+\\b$")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, ".+\\b$")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, ".+;$")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, ".+\\b;$")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "^[:punct:]")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "^:punct:")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "(?<=:punct:)")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, ":punct:")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "[:punct:]")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "[^:punct:]")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "[^+:punct:]")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "[^*:punct:]")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, "[:punct:]")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, ".+[:punct:]")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, ".*[:punct:]")
test_string <- "Hollie Silverman;Amir Vera"
str_extract(test_string, ".*[:punct:]")
str_extract(test_string, "[:punct:].*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, ".*[:punct:]")
str_extract(test_string, "[:punct:].*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, ".*^[:punct:]")
str_extract(test_string, "[:punct:].*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]")
str_extract(test_string, "[:punct:].*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*")
str_extract(test_string, "[:punct:].*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*[:blank:][:alpha:]*")
str_extract(test_string, "[:punct:].*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*[:blank:][:alpha:]*")
str_extract(test_string, "[:punct:].*[:alpha:]*[:blank:]")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*[:blank:][:alpha:]*")
str_extract(test_string, "[:punct:][:alpha:]*[:blank:][:alpha:]*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*[:blank:][:alpha:]*")
str_extract(test_string, "[:punct:][:alpha:]*[:blank:][:alpha:]*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*[:blank:][:alpha:]*")
str_extract(test_string, "[:punct:][:alpha:]*[:blank:][:alpha:]*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*[:blank:][:alpha:]*")
str_extract(test_string, "[:punct:][:alpha:]*[:blank:][:alpha:]*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*[:blank:][:alpha:]*")
str_extract(test_string, "[:;:][:alpha:]*[:blank:][:alpha:]*")
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*[:blank:][:alpha:]*")
str_extract(test_string, "[:punct:][:alpha:]*[:blank:][:alpha:]*")
cnn_data <- readRDS("CNN_cleaning.Rds")
cnn_data_nowith <- str_replace_all(cnn_data$authors, "with", ";")
cnn_data <- readRDS("CNN_cleaning.Rds")
cnn_data_nowith <- cnn_data %>%
select(title, authors, timestamp, body, url) %>%
mutate(
fixed_authors = str_replace_all(cnn_data$authors, "with", ";")
)
View(cnn_data_nowith)
View(cnn_data_nowith)
cnn_data <- readRDS("CNN_cleaning.Rds")
cnn_data_nowith <- cnn_data %>%
select(title, authors, timestamp, body, url) %>%
mutate(
fixed_authors = str_replace_all(cnn_data$authors, "with", ";")
)
n_vars <- cnn_data_nowith$fixed_authors %>% str_split(";") %>% lapply(function(z) length(z)) %>% unlist() %>% max()
n_vars
cnn_clean <- cnn_data_nowith %>%
select(title, fixed_authors, timestamp, body, url) %>%
mutate(separate(cnn_data_nowith, into = paste0("author_", 1:5), sep = ";", extra = "merge", fill = "right")
)
cnn_clean <- cnn_data_nowith %>%
select(title, fixed_authors, timestamp, body, url) %>%
mutate(separate(cnn_data_nowith$fixed_authors, into = paste0("author_", 1:5), sep = ";", extra = "merge", fill = "right")
)
cnn_clean <- cnn_data_nowith %>%
select(title, fixed_authors, timestamp, body, url) %>%
mutate(
separate(cnn_data_nowith$fixed_authors, into = paste0("author_", 1:5), sep = ";", extra = "merge", fill = "right")
)
cnn_clean <- cnn_data_nowith %>%
select(title, fixed_authors, timestamp, body, url) %>%
separate(cnn_data_nowith$fixed_authors, into = paste0("author_", 1:5), sep = ";", extra = "merge", fill = "right")
cnn_clean <- cnn_data_nowith %>%
separate(cnn_data_nowith$fixed_authors, into = paste0("author_", 1:5), sep = ";", extra = "merge", fill = "right")
cnn_clean <- cnn_data_nowith %>%
separate(cnn_data_nowith$fixed_authors, paste0("author_", 1:5), sep = ";", extra = "merge", fill = "right")
cnn_data <- readRDS("CNN_cleaning.Rds")
cnn_data_nowith <- cnn_data %>%
select(title, authors, timestamp, body, url) %>%
mutate(
fixed_authors = str_replace_all(cnn_data$authors, "with", ";")
)
cnn_data_nowith2 %>% mutate(fixed_authors = strsplit(, ";")) %>%
unnest(fixed_authors) %>%
group_by(title)
cnn_data_nowith %>% mutate(fixed_authors = strsplit(, ";")) %>%
unnest(fixed_authors) %>%
group_by(title)
cnn_data_nowith %>% mutate(fixed_authors = strsplit(";")) %>%
unnest(fixed_authors) %>%
group_by(title)
cnn_data_nowith %>% mutate(fixed_authors = strsplit(fixed_authors, ";")) %>%
unnest(fixed_authors) %>%
group_by(title)
# mutate(row = row_number()) %>%
#spread(row, to)
View(cnn_data_nowith)
View(cnn_data_nowith)
attempt <- cnn_data_nowith %>% mutate(fixed_authors = strsplit(fixed_authors, ";")) %>%
unnest(fixed_authors) %>%
group_by(title)
# mutate(row = row
View(attempt)
View(attempt)
attempt <- cnn_data_nowith %>% mutate(fixed_authors = strsplit(fixed_authors, ";")) %>%
unnest(fixed_authors) %>%
group_by(cnn_data_nowith$title)
test_string <- "Hollie Silverman;Amir Vera;Dinkus Winkus"
str_extract(test_string, "[:alpha:]*[:blank:][:alpha:]*")
str_extract(test_string, "[:punct:][:alpha:]*[:blank:][:alpha:]*")
cnn_clean <- cnn_data_nowith %>%
select(title, authors, timestamp, body, url) %>%
mutate(
lead_author = str_extract(cnn_data, "[:alpha:]*[:blank:][:alpha:]*")
)
cnn_clean <- cnn_data_nowith %>%
select(title, authors, timestamp, body, url) %>%
mutate(
lead_author = str_extract(cnn_data, "[:alpha:]*[:blank:][:alpha:]*")
)
cnn_clean <- cnn_data %>%
mutate(authors = str_replace_all(authors, " with ", ";")) %>%
separate_wider_delim(authors,
delim = ";",
names_sep = "",
too_few = "align_start",
cols_remove = TRUE) %>%
rename_with(~sub("authors", "author", .), starts_with("authors"))
cnn_clean <- cnn_data %>%
mutate(authors = str_replace_all(authors, " with ", ";")) %>%
separate_wider_delim(authors,
delim = ";",
names_sep = "",
too_few = "align_start",
cols_remove = TRUE) %>%
rename_with(~sub("authors", "author", .), starts_with("authors"))
cnn_data <- readRDS("CNN_cleaning.Rds")
cnn_clean <- cnn_data %>%
mutate(authors = str_replace_all(authors, " with ", ";")) %>%
separate_wider_delim(authors,
delim = ";",
names_sep = "",
too_few = "align_start",
cols_remove = TRUE) %>%
rename_with(~sub("authors", "author", .), starts_with("authors"))
View(cnn_clean)
View(cnn_clean)
str_detect(cnn_clean$timestamp, "Updated")
str_detect(cnn_clean$timestamp, "Updated")
cnn_clean2 <- cnn_data %>%
mutate(updated = str_detect(cnn_clean$timestamp, "Updated")) %>%
separate_wider_delim(updated,
delim = "",
names_sep = "",
too_few = "align_start",
cols_remove = TRUE)
library(dplyr)
library(quanteda)
library(dplyr)
library(quanteda)
install.packages(quanteda)
install.packages(c("quanteda", "quanteda.textstats", "quanteda.textmodels", "quanteda.textplots"))
library(dplyr)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
data_fox <- readRDS("data/FOX.Rds")
data_fox <- readRDS("FOX.Rds")
data_cnn <- readRDS("CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
View(data_news)
View(data_news)
set.seed(161)
news_sample <- slice_sample(data_news, n = 300)
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
library(dplyr)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
set.seed(161)
news_sample <- slice_sample(data_news, n = 300)
View(data_news)
df <- corpus(news_sample, text_field = "body")
summary(news_sample, n = 25)
df <- corpus(news_sample$, text_field = "body")
df <- corpus(news_sample, text_field = "body")
summary(df, n = 25)
df <- corpus(news_sample, text_field = "body")
summary(df, n = 50)
dfm(news_sample)
x <- tokens(df, remove_punct = FALSE, stem = FALSE)
x
y <- dfm(x)
y
textstat_frequency(x) | topfeatures(x)
textstat_frequency(y) | topfeatures(y)
textstat_frequency(y)
topfeatures(x)
topfeatures(y)
x <- tokens(df, remove_punct = TRUE, stem = FALSE)
x
x <- tokens(df, remove_punct = FALSE, stem = FALSE)
x
big_df <- corpus(data_news, text_field = "body")
# xy <- tokens(big_df, remove_punct = TRUE, stem = FALSE)
tokens_news <- tokens(
big_df,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en"), padding = TRUE)
big_dfm <- dfm(tokens_news)
textstat_frequency(big_dfm, n = 20)
big_df <- corpus(data_news, text_field = "body")
# xy <- tokens(big_df, remove_punct = TRUE, stem = FALSE)
tokens_news <- tokens(
big_df,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en"), padding = FALSE)
big_dfm <- dfm(tokens_news)
textstat_frequency(big_dfm, n = 20)
tfidf <- dfm_tfidf(big_dfm)
topfeatures(tfidf, n = 5, groups = docnames(tfidf))
tfidf <- dfm_tfidf(big_dfm)
topfeatures(tfidf, n = 5, groups = docnames(tfidf))
tfidf <- dfm_tfidf(small_dfm)
small_dfm <- dfm(x)
tfidf <- dfm_tfidf(small_dfm)
topfeatures(tfidf, n = 5, groups = docnames(tfidf))
tfidf <- dfm_tfidf(small_dfm)
topfeatures(tfidf, n = 5)
kwic(tokens_news, "protest", window = 5)
sample_tokens <- tokens(df, remove_punct = FALSE, stem = FALSE)
small_corpus <- corpus(news_sample, text_field = "body")
tokens_sample <- tokens(
small_corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en"), padding = FALSE)
textstat_collocations(tokens_sample, size = 10)
textstat_collocations(tokens_sample, size = 3)
kwic(tokens_news, "protest", window = 5)
textstat_collocations(tokens_sample, size = 2, n = 5)
small_corpus <- corpus(news_sample, text_field = "body")
tokens_sample <- tokens(
small_corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
# tokens_wordstem(language = "en") |>
tokens_remove(stopwords("en"), padding = FALSE)
#kwic(tokens_news, "protest", window = 5)
textstat_collocations(tokens_sample, size = 4)
fcm_pp <- fcm(tokens_sample, context = "window", count = "frequency", window = 3)
fcm_pp
topfeatures(fcm_pp)
summary(fcm_pp)
col <- tokens_sample |>
tokens_remove(stopwords("en")) |>
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = FALSE, padding = TRUE) |>
textstat_collocations(min_count = 5, tolower = FALSE)
head(col)
col <- small_corpus |>
tokens_remove(stopwords("en")) |>
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = FALSE, padding = TRUE) |>
textstat_collocations(min_count = 5, tolower = FALSE)
sample_dirty_tokens <- tokens(df, remove_punct = FALSE, stem = FALSE)
col <- small_dirty_tokens |>
tokens_remove(stopwords("en")) |>
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = FALSE, padding = TRUE) |>
textstat_collocations(min_count = 5, tolower = FALSE)
sample_dirty_tokens <- tokens(df, remove_punct = FALSE, stem = FALSE)
col <- sample_dirty_tokens |>
tokens_remove(stopwords("en")) |>
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = FALSE, padding = TRUE) |>
textstat_collocations(min_count = 5, tolower = FALSE)
head(col)
#kwic(tokens_news, "protest", window = 5)
textstat_collocations(tokens_sample, size = 4)
head(15)
#kwic(tokens_news, "protest", window = 5)
colo <- textstat_collocations(tokens_sample, size = 4)
head(15)
#kwic(tokens_news, "protest", window = 5)
colo <- textstat_collocations(tokens_sample, size = 4) |>
head(15)
#kwic(tokens_news, "protest", window = 5)
textstat_collocations(tokens_sample, size = 4) |>
head(15)
library(dplyr)
library(quanteda)
library(quanteda.sentiment)
remotes::install_github("quanteda/quanteda.sentiment")
library(dplyr)
library(quanteda)
library(quanteda.sentiment)
library(seededlda)
library(quanteda.textmodels)
data_fox <- readRDS("data/FOX.Rds")
data_fox <- readRDS("data/FOX.Rds")
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
set.seed(161)
data_sample300 <- slice_sample(data_news, n = 300)
set.seed(161)
data_sample300 <- slice_sample(data_news, n = 300)
corpus <- corpus(news_sample, text_field = "body")
tokens_raw <- tokens(corpus)
tokens_pp <- tokens(
corpus,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(stopwords("en"), padding = FALSE) |> # show padding
tokens_wordstem(language = "en")
dfm_pp <- dfm(tokens_pp)
head(dfm_pp)
dict_pol <- data_dictionary_HuLiu
dfm_lookup(dfm_pp, dict_pol)
textstat_polarity(dfm_pp, dict_pol)
textstat_valence(dfm_pp, data_dictionary_AFINN)
# Do Sentiment Analysis
dict_pol <- data_dictionary_HuLiu
dfm_lookup(dfm_pp, dict_pol)
#textstat_polarity(dfm_pp, dict_pol)
#textstat_valence(dfm_pp, data_dictionary_AFINN)
# Do Sentiment Analysis
dict_pol <- data_dictionary_HuLiu
# dfm_lookup(dfm_pp, dict_pol)
textstat_polarity(dfm_pp, dict_pol)
#textstat_valence(dfm_pp, data_dictionary_AFINN)
# Do Sentiment Analysis
dict_pol <- data_dictionary_HuLiu
# dfm_lookup(dfm_pp, dict_pol)
#textstat_polarity(dfm_pp, dict_pol)
textstat_valence(dfm_pp, data_dictionary_AFINN)
# Do Sentiment Analysis
dict_pol <- data_dictionary_HuLiu
# dfm_lookup(dfm_pp, dict_pol)
textstat_polarity(dfm_pp, dict_pol)
#textstat_valence(dfm_pp, data_dictionary_AFINN)
# Do Sentiment Analysis
dict_pol <- data_dictionary_HuLiu
# dfm_lookup(dfm_pp, dict_pol)
df <-  textstat_polarity(dfm_pp, dict_pol)
df
#textstat_valence(dfm_pp, data_dictionary_AFINN)
View(big_dfm)
View(df)
# Intro Steps: Import Data, Create Tokens and DFM Object
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")
data_news <- bind_rows(
list("fox" = data_fox, "cnn" = data_cnn),
.id = "source"
)
#set.seed(161)
#data_sample300 <- slice_sample(data_news, n = 300)
corpus <- corpus(data_news, text_field = "body")
tokens_raw <- tokens(corpus)
tokens_pp <- tokens(
corpus,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_separators = TRUE
) |>
tokens_tolower() |>
tokens_remove(stopwords("en"), padding = FALSE) |> # show padding
tokens_wordstem(language = "en")
dfm_pp <- dfm(tokens_pp)
head(dfm_pp)
corpus %>%
corpus_subset(source == "fox") %>%
dfm(remove = stopwords("english")) %>%
textplot_wordcloud()
corpus %>%
corpus_subset(source == "fox") %>%
dfm_remove(stopwords("english")) %>%
textplot_wordcloud()
quanteda.textmodels::data_corpus_EPcoaldebate
dfm_pp <- dfm(tokens_pp) |> dfm_trim(min_termfreq = 4)
head(dfm_pp)
tmod_lda <- textmodel_lda(dfm_pp, k = 3)
dfmat_news <- dfm(tokens_pp) %>%
dfm_trim(min_termfreq = 0.8, termfreq_type = "quantile",
max_docfreq = 0.1, docfreq_type = "prop")
tmod_lda <- textmodel_lda(dfmat_news, k = 5)
gc()
#tmod_lda <- textmodel_lda(dfmat_news, k = 5)
#terms(tmod_lda, 10)
dict_topic <- dictionary(file = "/data/LaverGarry.rda")
#tmod_lda <- textmodel_lda(dfmat_news, k = 5)
#terms(tmod_lda, 10)
dict_topic <- readRDA("/data/LaverGarry.rda")
#tmod_lda <- textmodel_lda(dfmat_news, k = 5)
#terms(tmod_lda, 10)
dict_topic <- read("/data/LaverGarry.rda")
#tmod_lda <- textmodel_lda(dfmat_news, k = 5)
#terms(tmod_lda, 10)
dict_topic <- load("/data/LaverGarry.rda")
#tmod_lda <- textmodel_lda(dfmat_news, k = 5)
#terms(tmod_lda, 10)
dict_topic <- load("data/LaverGarry.rda")
head(dict_topic)
