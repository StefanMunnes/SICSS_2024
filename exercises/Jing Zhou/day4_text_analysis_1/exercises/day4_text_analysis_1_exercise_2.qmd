---
title: "Text Analysis I - Exercise I"
date: July 11, 2024
format: 
  html:
    embed-resources: true
    number-sections: true
editor: visual
execute: 
  eval: false
  echo: true
  output: false
  warning: false
  error: false
---

# Preparation

Packages you will (probably) need:

```{r}
remotes::install_github("quanteda/quanteda.sentiment")


library(dplyr)
library(quanteda)
library(quanteda.sentiment)
library(seededlda)
library(quanteda.textmodels)
```

Use the script from exercise 1 to get all the necessary objects for the analysis

# Perform a sentiment analysis

Get a dictionary from the quanteda.sentiment package and calculate the polarity (or valence) sentiment for each document:

```{r}
data_fox <- readRDS("data/FOX.Rds")
data_cnn <- readRDS("data/CNN.Rds")

data_news <- bind_rows(
  list("fox" = data_fox, "cnn" = data_cnn),
  .id = "source"
)

set.seed(161)

data_news <- slice_sample(data_news, n = 300)

corpus_news <- corpus(data_news$body)
summary_corpus_news <- summary(corpus_news)
summary_corpus_news
tokens_news <- tokens(corpus_news, 
                 what = "word", 
                 remove_punct = TRUE, 
                 remove_symbols = TRUE, 
                 remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en")) %>%
  tokens_wordstem()
# Create a Document-Feature Matrix (DFM)
dfm_news <- dfm(tokens_news)

# Inspect the DFM
print(dfm_news)


dict_pol <- data_dictionary_HuLiu
polarity<-textstat_polarity(dfm_news, dict_pol)
polarity
valence<-textstat_valence(dfm_news, data_dictionary_AFINN)
valence


```

# Perform a topic models analysis

Use the quantda.texmodel function for a explorativ approach or define some seeds if you expect some topics with seeded lda:

```{r}
library(quanteda.textmodels)
library(seededlda)

tmod_lda <- textmodel_lda(dfm_news, k = 12)

terms(tmod_lda, 10)

topics<-topics(tmod_lda)
topics
```

# **Bonus**: Train your own naive bayes classifier for sentiment

Get a training set from the labeled data and define classes:

```{r}
data <- read.csv("data/IMDB Dataset.csv", header = TRUE, stringsAsFactors = FALSE)
# Check the structure and first few rows of the dataset
head(data)

set.seed(123)  # Set seed for reproducibility
train_indices <- sample(nrow(data), 0.8 * nrow(data))  # 80% for training
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

classes <- unique(data$sentiment)
classes

summary(train_data)
summary(test_data)

```

Train the model with the training data and classes:

```{r}
library(quanteda)
# Create a corpus and tokens from training data
corpus_train <- corpus(train_data$review)
tokens_train <- tokens(corpus_train)

# Convert tokens to a document-feature matrix (dfm)
dfm_train <- dfm(tokens_train)

# Train a Naive Bayes classifier
model_nb <- textmodel_nb(dfm_train, classes)
```

Get (unlabelled) test data:

```{r}
# Preprocess test data
corpus_test <- corpus(test_data$review)
tokens_test <- tokens(corpus_test)

tokens_test <- tokens(corpus_test, 
                 what = "word", 
                 remove_punct = TRUE, 
                 remove_symbols = TRUE, 
                 remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en")) 

dfm_test <- dfm(tokens_test, dfm_train)  # Use the same features as in training

# Predict sentiment labels
predictions <- predict(model_nb, dfm_test)

# Evaluate the model (e.g., accuracy)
accuracy <- mean(predictions == test_data$sentiment)

```

Predict class for test data with trained model:

```{r}

```

Create a simple confusion matrix to check prediction accuracy:

```{r}

```
